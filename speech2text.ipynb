{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting CUDA device if available and changing default fork strategy to spawn as it works better on UNIX systems(if you have Windows, switching back to fork is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing audio to mel-spectrograms and text to be tokenized according to token_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, dataset_name : str, token_vocabulary : list, dataset_split : str = 'train'):\n",
    "        super().__init__()\n",
    "        self.dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "        self.token_vocabulary = token_vocabulary\n",
    "\n",
    "    def play_audio(self, idx : int):\n",
    "        item = self.dataset[idx]\n",
    "        audio_array = item['audio']['array']\n",
    "        sampling_rate = item['audio']['sampling_rate']\n",
    "        return ipd.Audio(audio_array, rate=sampling_rate, autoplay=True)\n",
    "\n",
    "    def audio_to_mel(self, audio_array : np.ndarray, sample_rate : int, n_mels : int = 80, target_sample_rate : int = 16000) -> np.array:\n",
    "        if sample_rate != target_sample_rate:\n",
    "            audio_array = librosa.resample(audio_array, orig_sr=sample_rate, target_sr=target_sample_rate)\n",
    "        mel_spectr = librosa.feature.melspectrogram(y=audio_array, sr=target_sample_rate, n_mels=n_mels)\n",
    "        log_mel_spectr = librosa.power_to_db(mel_spectr, ref=np.max)\n",
    "        log_mel_spectr_norm = librosa.util.normalize(log_mel_spectr, axis=0)\n",
    "        return log_mel_spectr_norm\n",
    "\n",
    "    def text_to_tokens(self, text : str) -> list[int]:\n",
    "        text_clean = re.sub(r\"[^a-z\\s]\", '', text.lower())\n",
    "        tokens = [self.token_vocabulary.index(letter) for letter in text_clean]\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        audio_array = item['audio']['array']\n",
    "        sampling_rate = item['audio']['sampling_rate']\n",
    "        mel_spectrogram = self.audio_to_mel(audio_array, sampling_rate)\n",
    "\n",
    "        text = item['text']\n",
    "        text_tokens = self.text_to_tokens(text)\n",
    "\n",
    "        output = {\n",
    "            'input_values' : mel_spectrogram,\n",
    "            'text_tokens' : torch.Tensor(text_tokens)\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using english alphabet and whitespace as model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY = list(' ' + string.ascii_lowercase)\n",
    "VOCAB_SIZE = len(VOCABULARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading it takes few hours as it consists of 399 Gb of data, after that it takes few minutes to load from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = ASRDataset(\"MLCommons/peoples_speech\", VOCABULARY, 'train')\n",
    "test_ds = ASRDataset(\"MLCommons/peoples_speech\", VOCABULARY, 'test')\n",
    "val_ds = ASRDataset(\"MLCommons/peoples_speech\", VOCABULARY, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and decode functions to test the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    text_clean = re.sub(r\"[^a-z\\s]\", '', text.lower())\n",
    "    tokens = [VOCABULARY.index(letter) for letter in text_clean]\n",
    "    return tokens\n",
    "\n",
    "def decode_text(text_tokens):\n",
    "    decoded_text = \"\".join(list(map(lambda x : VOCABULARY[int(x)], text_tokens)))\n",
    "    text_formatted = decoded_text.rstrip().lstrip()\n",
    "    return text_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode_text(train_ds[0]['text_tokens']))\n",
    "train_ds.play_audio(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_ds[0]['input_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing dataloader for training process. Input values would be padded to same length batch-wise. Lengths are needed for CTC loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features = []\n",
    "    labels = []\n",
    "    features_lengths = []\n",
    "    labels_lengths = []\n",
    "    for sample in batch:\n",
    "        features.append(torch.Tensor(sample['input_values']).permute(1, 0))\n",
    "        features_lengths.append(features[-1].shape[0])\n",
    "\n",
    "        labels.append(sample['text_tokens'])\n",
    "        labels_lengths.append((sample['text_tokens'] != 0).sum().item())\n",
    "    features = pad_sequence(features, batch_first=True).permute(0, 2, 1)\n",
    "    labels = pad_sequence(labels, batch_first=True)\n",
    "    return features, labels, features_lengths, labels_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "dataloader_train = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "dataloader_val = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "dataloader_test = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_feature, sample_target, sample_lengths, target_lengths in dataloader_train:\n",
    "    plt.imshow(sample_feature[0])\n",
    "    print(decode_text(sample_target[1]))\n",
    "    print(sample_lengths)\n",
    "    print(target_lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying speech-to-text Conformer inspired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, input_dim : int, output_dim : int):\n",
    "        super(FeedForwardModule, self).__init__()\n",
    "        self.linear_relu = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(output_dim*4, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class ConformerModule(nn.Module):\n",
    "    def __init__(self, embedding_dim : int, output_dim : int, num_heads : int, conv_kernel_size : int = 5, dropout_rate : float = 0.1):\n",
    "        super(ConformerModule, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=num_heads)\n",
    "        self.feed_forward1 = FeedForwardModule(embedding_dim, embedding_dim)\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, embedding_dim*2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(embedding_dim*2, embedding_dim, conv_kernel_size, padding=(conv_kernel_size-1)//2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.feed_forward2 = FeedForwardModule(embedding_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        x = x + self.dropout(attention_output)\n",
    "\n",
    "        ff_output1 = self.feed_forward1(x)\n",
    "        x = x + self.dropout(ff_output1)\n",
    "\n",
    "        x = x.permute(1, 2, 0)\n",
    "        conv_output = self.conv_block(x)\n",
    "        x = x + self.dropout(conv_output)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        ff_output2 = self.feed_forward2(x)\n",
    "        x = x + self.dropout(ff_output2)\n",
    "        return x\n",
    "\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, n_mels : int, hidden_dim : int, vocab_size : int, dropout_rate : float = 0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, hidden_dim, 1)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.conformer_block = nn.Sequential(\n",
    "            ConformerModule(hidden_dim, hidden_dim, 8, dropout_rate=dropout_rate),\n",
    "            ConformerModule(hidden_dim, hidden_dim, 8, dropout_rate=dropout_rate),\n",
    "            ConformerModule(hidden_dim, hidden_dim, 8, dropout_rate=dropout_rate),\n",
    "            ConformerModule(hidden_dim, hidden_dim, 8, dropout_rate=dropout_rate),\n",
    "            ConformerModule(hidden_dim, hidden_dim, 8, dropout_rate=dropout_rate)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.conformer_block(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASRModel(80, 512, VOCAB_SIZE).to(device)\n",
    "\n",
    "summary(model, (80, 1), batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss = nn.CTCLoss(blank=0)\n",
    "\n",
    "def ctc_loss_fn(y_true, y_pred, target_lengths, pred_lengths):\n",
    "    y_pred = torch.clamp(y_pred, min=1e-7)\n",
    "    y_pred_log_softmax = nn.LogSoftmax(dim=2)(y_pred)\n",
    "    loss = ctc_loss(y_pred_log_softmax, y_true, pred_lengths, target_lengths)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"loss\" : [float('inf')],\n",
    "    \"val_loss\" : [float('inf')],\n",
    "    \"test_loss\" : [float('inf')]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dataset and model are considerably large, we may encounter situations when VRAM is going to be overfitted. To avoid interrupting training process we'll ignore batches which causes overfitting, also we are going to use scaler and autocast in order to reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "def train(model : nn.Module, dataloader : DataLoader, display_each : int = 20):\n",
    "    global history\n",
    "    out_of_mem = not_lost = 0\n",
    "    accumulation_steps = 4\n",
    "    loss_avg = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(dataloader, desc=f\"loss: {history['loss'][-1]:.2f}, out_of_mem: {out_of_mem/(not_lost + out_of_mem + 1e-6):.2f}\")\n",
    "    for batch_idx, (features_batch, targets_batch, feature_lengths, target_lengths) in enumerate(pbar):\n",
    "        try:\n",
    "            features_batch, targets_batch = features_batch.to(device, non_blocking=True), targets_batch.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                pred = model(features_batch)\n",
    "                loss = ctc_loss_fn(targets_batch, pred, target_lengths, feature_lengths)\n",
    "                loss_avg += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx+1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            not_lost += 1\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                out_of_mem += 1\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        if (batch_idx+1) % display_each == 0:\n",
    "            history['loss'].append(loss_avg/display_each)\n",
    "            pbar.set_description(f\"loss: {loss_avg/display_each:.2f}, out_of_mem_ratio: {out_of_mem/(not_lost + out_of_mem):.4f}\")\n",
    "            loss_avg = 0\n",
    "\n",
    "        del features_batch, targets_batch, feature_lengths, target_lengths\n",
    "\n",
    "def validate(model : nn.Module, dataloader : DataLoader, display_each : int = 20):\n",
    "    global history\n",
    "    out_of_mem = not_lost = 0\n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"val_loss: {history['val_loss'][-1]}\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features_batch, targets_batch, feature_lengths, target_lengths) in enumerate(pbar):\n",
    "            try:\n",
    "                features_batch, targets_batch = features_batch.to(device, non_blocking=True), targets_batch.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast('cuda'):\n",
    "                    pred = model(features_batch)\n",
    "                    loss = ctc_loss_fn(targets_batch, pred, target_lengths, feature_lengths)\n",
    "                    loss_avg += loss.item()\n",
    "\n",
    "                not_lost += 1\n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    out_of_mem += 1\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "            if (batch_idx+1) % display_each == 0:\n",
    "                history['val_loss'].append(loss_avg/display_each)\n",
    "                pbar.set_description(f\"val_loss: {loss_avg/display_each:.2f}, out_of_mem_ratio: {out_of_mem/(not_lost + out_of_mem):.4f}\")\n",
    "                loss_avg = 0\n",
    "\n",
    "            del features_batch, targets_batch, feature_lengths, target_lengths\n",
    "\n",
    "def test(model : nn.Module, dataloader : DataLoader, display_each : int = 20):\n",
    "    global history\n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"test_loss: {history['test_loss'][-1]:.2f}\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features_batch, targets_batch, feature_lengths, target_lengths) in enumerate(pbar):\n",
    "            features_batch, targets_batch = features_batch.to(device, non_blocking=True), targets_batch.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                pred = model(features_batch)\n",
    "                loss = ctc_loss_fn(targets_batch, pred, target_lengths, feature_lengths)\n",
    "                loss_avg += loss.item()\n",
    "\n",
    "            if (batch_idx+1) % display_each == 0:\n",
    "                pbar.set_description(f\"test_loss: {loss_avg/display_each:.4f}\")\n",
    "                history['test_loss'].append(loss_avg/display_each)\n",
    "                loss_avg = 0\n",
    "\n",
    "            del features_batch, targets_batch, feature_lengths, target_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the training process. After each epoch of training we'll save model if it outperforms itself on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoint'\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"Epoch : {i+1}/{EPOCHS}\")\n",
    "    train(model, dataloader_train)\n",
    "    validate(model, dataloader_val)\n",
    "    current_val_loss = history['val_loss'][-1]\n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        checkpoint_file = os.path.join(checkpoint_path, f'asr_conformer_best.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"New best model saved with val_loss: {best_val_loss:.4f} at {checkpoint_file}\")\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss ({current_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(mel_spec):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.Tensor([mel_spec]).to(device))\n",
    "        pred = nn.Softmax(dim=2)(pred)\n",
    "    chosen_tokens = torch.argmax(pred, dim=2)\n",
    "    return chosen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 200\n",
    "print(decode_text(transcribe(test_ds[i]['input_values'])))\n",
    "plt.imshow(test_ds[i]['input_values'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.play_audio(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA memory summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
